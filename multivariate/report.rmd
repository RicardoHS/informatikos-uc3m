---
title: "Statistics for Data Science - UC3M Master"
subtitle: "Multivariate Analysis - First Assignment"
author: 
  - "Ricardo Hortelano"
  - "Javier Martinez"
  - "Santiago Raposo"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{bm}
output:
  pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = NA,
                      dev = 'png',
                      dpi = 300)
library(plotly)
library(tidyverse)
library(MASS)
library(pracma)
library(corrplot)
library(GGally)
library(factoextra)
library(ggrepel)
library(robustbase)
library(cowplot)
theme_set(theme_bw())
```

```{r poke, include = FALSE, cache = TRUE}
poke <- read_csv("data/pokemon.csv")[,-1] %>% 
  mutate_at(vars(contains("number"), "target", contains("Generation")), as.factor)

poke_long <- poke %>%
  dplyr::select(-target) %>%
  pivot_longer(
    everything(),
    names_to = c("fight", ".value"),
    names_sep = "_"
  ) %>% 
  distinct(Name, .keep_all = T)
```


# Pre-process the data set for practical analysis.

## Missing Values
The only variables with missing values are `poke1_Type.2` and `poke2_Type.2`. Although these values do not exist, they are not considered as actual missing values. Any given pokemon can have up to two types, the second being optional. Therefore, the absence of this second type is treated, not as unrecorded data, but as a type in itself.

Regarding the rest of the dataset, there are no missing values.

# Distribution of Statistics

### Single Variable Analysis
For some of our plots and exploratory analysis, we are only interested in the distribution of the variables of the pokemons. Since each observation of our dataset is composed of two pokemon, we have joined the distributions of each of their variables in the dataset `poke_long`

#### Density Plots
First we can see some density plots of our numerical variables to get some insight into their shape:

```{r}
poke_long %>% 
  select_if(is.numeric) %>% 
  pivot_longer(
    everything(),
    names_to = "Statistic",
    values_to = "Value"
  ) %>% 
  # mutate(Value = log(Value)) %>% 
  ggplot(aes(x = Value, y = ..density..))+ 
  geom_density(fill = "lightblue")+
  facet_wrap(~Statistic)+
  theme_bw()
```

We can also try to get some insight into the distribution of our numerical variables over the values of our categorical variables by plotting some bar charts.

```{r}
poke_long %>% 
  ggplot(aes(x = Generation))+
  geom_bar()+
  theme_bw()
```

We can see that the distribution of the generations of pokemon is not uniform, which might affect the amount of wins for each generation in the fights. We can also check the distribution of the legendary pokemon in our data.

We can also look at a boxplot of our variables to get some more insight into their distributions, as well as letting us see wheter there are any outliers in our data.
```{r, cache = TRUE, dependson="poke"}
poke_long %>% 
  select_if(is.numeric) %>% 
  pivot_longer(
    everything(),
    names_to = "Statistic",
    values_to = "Value"
  ) %>% 
  ggplot(aes(x = Statistic, y = Value))+
  geom_boxplot()+
  coord_flip()+
  theme_bw()
```
As we can see, there are some extreme cases present in the boxplot. Nevertheless, we cannot treat this extreme cases as outliers due the fact that a pokemon with extreme values in one variable can have normal values in other variables.

We know that there are two groups in our dataset, as we have the variable `Legendary` which indicates if a pokemon has that quality. It is also known that legendary pokemon have higher values for their statistics than standard ones, but we might want to check if it is true.

A way we can verify this statement is by looking at a scatter plot matrix and the density of our variables when separated by this categorical variable.
```{r, cache=TRUE, dependson="poke", message = F}
numeric_vars <- poke_long %>% 
  select_if(~is.numeric(.) || is.logical(.))
ggpairs(numeric_vars, aes(color = Legendary))
```
Indeed, legendary pokemon seem to have higher values for their variables in general than normal pokemon. This means that we will have to treat legendary pokemon separately for our outlier detection, as otherwise they will be found to be outliers (when they are really just a different group).

We can also see the difference between legendary and normal pokemon in a parallel coordinates plot.
```{r, fig.height = 3, cache = T, dependson = "poke"}
ggparcoord(numeric_vars, groupColumn = "Legendary")+
  theme_bw()
```

We can see that legendary pokemon indeed have in general higher values for their stats than normal pokemon, as is to be expected. We might be also interested in seeing if there is any difference between the pokemon from different generations with respect to their statistics, for which we can also use a parallel coordinates plot.
```{r}
as <- bind_cols(numeric_vars, Generation= poke_long$Generation)
ggparcoord(as, columns = 1:7, groupColumn = "Generation") + theme_bw()
```
We can see that, unlike in the case of legendary pokemon, there does not appear to be any particular generation that stands out among the rest of them.

## Outlier Detection
In order to check if there are any outliers in our dataset we will first remove the legendary pokemon from consideration. After this, we will employ the false False Discovey Rate Method to check for outliers among the remaining pokemon.
```{r}
poke_x <- poke_long %>%
  filter(Legendary == 0) %>% 
  select_if(is.numeric)
n <- nrow(poke_x)
p <- ncol(poke_x)
mah_poke <- mahalanobis(poke_x, colMeans(poke_x), cov(poke_x))
plot(sort(mah_poke), main = "Mahalanobis Distance", xlab = "", ylab = "")
```
The extreme cases in the right side can be outliers. In order to check that, we can compute the outliers using the $\chi^2$ distribution test:
```{r}
p_values <- 1 - pchisq(mah_poke, p)
sort_index <- order(p_values)
outliers_index <- sort_index[which(p_values[sort_index] < ((1:n)/n*0.01))]

poke_long[outliers_index, ] %>% 
  dplyr::select(Name)
```

As there are indeed some outliers, we can try to compute the Mahalanobis distance again using the robust estimators for the sample covariance matrix.
```{r}
mcd_poke <- covMcd(poke_x, alpha = .9)
mah_robust <- mahalanobis(poke_x, mcd_poke$center, mcd_poke$cov)

plot(sort(mah_robust), main = "Mahalanobis Distance (Robust Estimate)", xlab = "", ylab= "")
```

And for the calculation of p-values and detection of outliers:
```{r}
p_values_robust <- 1 - pchisq(mah_robust, p)
sort_index_robust <- order(p_values_robust)
outliers_index_robust <- 
  sort_index_robust[which(p_values_robust[sort_index_robust] < ((1:n)/n*0.01))]

poke_long[outliers_index, ] %>% dplyr::select(Name)
```
We will not be removing these data points, as they are completely fine pokemon, but we will have to make sure that we are using robust estimators in our analyses.

# Principal Component Analysis

For principal component analysis we will use the whole dataset. Obviously we can only consider the numerical variables in our dataset.

```{r}
poke_numeric <- select_if(poke, is.numeric) 
pca <- prcomp(poke_numeric)
fviz_eig(pca, ncp = 12)
```
We can see in this plot that the first two principal components already explain almost half the variance in our dataset. We will now explore the weights of the principal components.

We can plot the weights of the first three principal components to get some insight into our data.
```{r}
pca_weights <- as_tibble(pca$rotation, rownames = "variable")
ggplot(pca_weights, aes(x = 1:nrow(pca_weights), y = PC1))+
  geom_point()+
  geom_label_repel(aes(label = variable))+
  geom_hline(yintercept=0)
```

```{r}
ggplot(pca_weights, aes(x = 1:nrow(pca_weights), y = PC2))+
  geom_point()+
  geom_label_repel(aes(label = variable))+
  geom_hline(yintercept=0)
```

```{r}
ggplot(as.data.frame(pca$rotation[,1:2]), aes(x=0,y=0,xend=PC1, yend=PC2, label=rownames(as.data.frame(pca$rotation[,1:2])))) + geom_segment( arrow = arrow(length = unit(0.1,"cm"))) + coord_cartesian(xlim=-1:1, ylim=-1:1) + geom_text()
```




