---
title: "Statistics for Data Science - UC3M Master"
subtitle: "Multivariate Analysis - First Assignment"
author: 
  - "Ricardo Hortelano"
  - "Javier Mart√≠nez"
  - "Santiago Raposo"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{bm}
output:
  pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      comment = NA,
                      fig.height = 4,
                      fig.align = "center")
library(plotly)
library(tidyverse)
library(MASS)
library(pracma)
library(corrplot)
library(GGally)
library(factoextra)
library(ggrepel)
library(robustbase)
library(cowplot)
library(cluster)
library(mclust)
theme_set(theme_bw())
```

```{r poke, include = FALSE, cache = TRUE}
poke <- read_csv("data/pokemon.csv")[,-1] %>% 
  mutate_at(vars(contains("number"), "target", contains("Generation")), as.factor)

poke_long <- poke %>%
  dplyr::select(-target) %>%
  pivot_longer(
    everything(),
    names_to = c("fight", ".value"),
    names_sep = "_"
  ) %>% 
  distinct(Name, .keep_all = T)
```


# Pre-process the data set for practical analysis.

## Missing Values
The only variables with missing values are `poke1_Type.2` and `poke2_Type.2`. Although these values do not exist, they are not considered as actual missing values. Any given pokemon can have up to two types, the second being optional. Therefore, the absence of this second type is treated, not as unrecorded data, but as a type in itself.

Regarding the rest of the dataset, there are no missing values.

# Distribution of Statistics

### Single Variable Analysis
For some of our plots and exploratory analysis, we are only interested in the distribution of the variables of the pokemons. Since each observation of our dataset is composed of two pokemon, we have joined the distributions of each of their variables in the dataset `poke_long`

#### Density Plots
First we can see some density plots of our numerical variables to get some insight into their shape:

```{r fig.height=3}
poke_long %>% 
  select_if(is.numeric) %>% 
  pivot_longer(
    everything(),
    names_to = "Statistic",
    values_to = "Value"
  ) %>% 
  # mutate(Value = log(Value)) %>% 
  ggplot(aes(x = Value, y = ..density..))+ 
  geom_density(fill = "lightblue")+
  facet_wrap(~Statistic)+
  theme_bw()
```

We can also try to get some insight into the distribution of our numerical variables over the values of our categorical variables by plotting some bar charts.

```{r fig.height=3}
poke_long %>% 
  ggplot(aes(x = Generation))+
  geom_bar()+
  theme_bw()
```

We can see that the distribution of the generations of pokemon is not uniform, which might affect the amount of wins for each generation in the fights.

In order to get some more insight into their distributions, we can also look at a boxplot of our variables, as well as letting us see wheter there are any outliers in our data.

```{r, cache = TRUE, dependson="poke", dev = "png", dpi = 300, fig.height=3.75}
poke_long %>% 
  select_if(is.numeric) %>% 
  pivot_longer(
    everything(),
    names_to = "Statistic",
    values_to = "Value"
  ) %>% 
  ggplot(aes(x = Statistic, y = Value))+
  geom_boxplot()+
  coord_flip()+
  theme_bw()
```

As we can see, there are some extreme cases present in the boxplot. Nevertheless, we cannot treat this extreme cases as outliers due the fact that a pokemon with extreme values in one variable can have normal values in other variables.

We know that there are two groups in our dataset, as we have the variable `Legendary` which indicates if a pokemon has that quality. It is also known that legendary pokemon have higher values for their statistics than standard ones, but we might want to check if it is true.

A way we can verify this statement is by looking at a scatter plot matrix and the density of our variables when separated by this categorical variable.

```{r, cache=TRUE, dependson="poke", message = F}
numeric_vars <- poke_long %>% 
  select_if(~is.numeric(.) || is.logical(.))
ggpairs(numeric_vars, aes(color = Legendary))
```

Indeed, legendary pokemon seem to have higher values for their variables in general than normal pokemon. This means that we will have to treat legendary pokemon separately for our outlier detection, as otherwise they will be found to be outliers (when they are really just a different group).

We can also see the difference between legendary and normal pokemon in a parallel coordinates plot.

```{r, fig.height = 3, cache = T, dependson = "poke"}
ggparcoord(numeric_vars, groupColumn = "Legendary")+
  theme_bw()
```

We can see that legendary pokemon indeed have in general higher values for their stats than normal pokemon, as is to be expected. We might be also interested in seeing if there is any difference between the pokemon from different generations with respect to their statistics, for which we can also use a parallel coordinates plot.
```{r}
as <- bind_cols(numeric_vars, Generation= poke_long$Generation)
ggparcoord(as, columns = 1:7, groupColumn = "Generation") + theme_bw()
```

We can see that, unlike in the case of legendary pokemon, there does not appear to be any particular generation that stands out among the rest of them.

## Outlier Detection

In order to check if there are any outliers in our dataset we will first remove the legendary pokemon from consideration. After this, we will employ the false False Discovey Rate Method to check for outliers among the remaining pokemon.

```{r, dev = "png", dpi = 300}
poke_x <- poke_long %>%
  filter(Legendary == 0) %>% 
  select_if(is.numeric)
n <- nrow(poke_x)
p <- ncol(poke_x)
mah_poke <- mahalanobis(poke_x, colMeans(poke_x), cov(poke_x))
plot(sort(mah_poke), main = "Mahalanobis Distance", xlab = "", ylab = "")
```

The extreme cases in the right side can be outliers. In order to check that, we can compute the outliers using the $\chi^2$ distribution test:

```{r}
p_values <- 1 - pchisq(mah_poke, p)
sort_index <- order(p_values)
outliers_index <- sort_index[which(p_values[sort_index] < ((1:n)/n*0.01))]

poke_long[outliers_index, ] %>% 
  dplyr::select(Name)
```

As there are indeed some outliers, we can try to compute the Mahalanobis distance again using the robust estimators for the sample covariance matrix.

```{r, dev  = "png", dpi = 300}
mcd_poke <- covMcd(poke_x, alpha = .9)
mah_robust <- mahalanobis(poke_x, mcd_poke$center, mcd_poke$cov)

plot(sort(mah_robust), main = "Mahalanobis Distance (Robust Estimate)", xlab = "", ylab= "")
```

And for the calculation of p-values and detection of outliers:

```{r}
p_values_robust <- 1 - pchisq(mah_robust, p)
sort_index_robust <- order(p_values_robust)
outliers_index_robust <- 
  sort_index_robust[which(p_values_robust[sort_index_robust] < ((1:n)/n*0.01))]

poke_long[outliers_index, ] %>% dplyr::select(Name)
```

We will not be removing these data points, as they are completely fine pokemon, but we will have to make sure that we are using robust estimators in our analyses.

# Principal Component Analysis

For principal component analysis we will use the whole dataset. Obviously we can only consider the numerical variables in our dataset.

```{r}
poke_numeric <- select_if(poke, is.numeric) 
pca <- prcomp(poke_numeric)
fviz_eig(pca, ncp = 12)
```

The shape of this plot seems odd, as it seems that principal components explain the variance of our dataset in pairs. To understand why, we can check the correlation plot for our numerical variables:

```{r}
corrplot(cor(poke_numeric))
```

Here we can see that the variables related to the two different pokemon in the fight are completely uncorrelated. This does make sense considering the dataset, but unfortunately it makes it so that the principal components that we would obtain from this dataset come in pairs (as each of the pairs could be explaining variability in each of the pokemon).

We can see that this is the case by plotting the weights of the first two principal components:

```{r, message = F, echo = F}
ggplot(as.data.frame(pca$rotation[,1:2]), 
       aes(x=0,y=0,
           xend=PC1, 
           yend=PC2, 
           label=rownames(as.data.frame(pca$rotation[,1:2]))))+ 
  geom_segment( arrow = arrow(length = unit(0.1,"cm")))+
  coord_cartesian(xlim=-1:1, ylim=-1:1)+
  geom_text(aes(x = PC1, y = PC2))+
  coord_flip()
```

In order to perform a proper analysis, we have transformed the dataset so that we only take into account the variables for one pokemon, although this does make it so that our dataset does not have many variables unfortunately.

```{r fig.height=3.5}
poke_numeric <- select_if(poke_long, is.numeric) 
pca <- prcomp(poke_numeric)
fviz_eig(pca, ncp = 12)
```

Note that we are not scaling our data before calculating the principal components, which means that we are performing the calculations using the *sample covariance* matrix. This is not a problem in our case as all of our variables have similar magnitudes and variances.

We can then proceed with the plots for the first three principal components so that we can assign some meaning into the obtained variables. This will not only help us interpret the meaning of the different principal components but will also be relevant when we perform cluster analysis later on in the report.

```{r fig.height=3.5}
pca_weights <- as_tibble(pca$rotation, rownames = "variable")
ggplot(pca_weights, aes(x = 1:nrow(pca_weights), y = PC1))+
  geom_point()+
  geom_label_repel(aes(label = variable))+
  geom_hline(yintercept=0)
```

We can see that the weights are all quite high in general, so perhaps this component just explains the general "power level" of a pokemon for instance.

```{r fig.height=3.5}
ggplot(pca_weights, aes(x = 1:nrow(pca_weights), y = PC2))+
  geom_point()+
  geom_label_repel(aes(label = variable))+
  geom_hline(yintercept=0)
```

For this second principal component we have that the weights for `Sp.Atk` and `Speed` are higher, so this might be related to the special offensive capabilities of the pokemon in some sense.

```{r fig.height=3.5}
ggplot(pca_weights, aes(x = 1:nrow(pca_weights), y = PC3))+
  geom_point()+
  geom_label_repel(aes(label = variable))+
  geom_hline(yintercept=0)
```

For this last PC, as the highest weight is related to the variable `Attack` this probably is related to the physical attack capabilities of the pokemon, as `Sp.Atk` and `Sp.Def` are related to non-physical combat type.

```{r, warning=F, message = F}
ggplot(as.data.frame(pca$rotation[,1:2]), 
       aes(x=0,y=0,
           xend=PC1, 
           yend=PC2, 
           label=rownames(as.data.frame(pca$rotation[,1:2]))))+ 
  geom_segment( arrow = arrow(length = unit(0.1,"cm")))+
  coord_cartesian(xlim=-1:1, ylim=-1:1)+
  geom_text(aes(x = PC1, y = PC2))+
  coord_flip()
```

# Factor Analysis

As we have seen in the previous section, we will limit our analysis to the variables of just one pokemon. We can first check the correlation between these variables by using again a correlation plot.

```{r, fig.height = 3, dev = "pdf", fig.align="center"}
poke_numeric <- select_if(poke_long, is.numeric)
corrplot(cor(poke_numeric), order = "hclust")
```

Unfortunately as there are not that many variables the correlation plot is not useful in order to if there are any groups present in the dataset. However one thing we can see that `Defense` and `Speed` are completely uncorrelated.

We can now calculate the $M$ matrix for the first 2 principal components, as we have only 6 variables so it does not make much sense to compute a lot of factors.

```{r}
poke_scaled <- scale(poke_numeric)
pca_factor <- prcomp(poke_scaled)

r <- 2
p <- ncol(poke_numeric)
M <-  pca_factor$rotation[,1:r] %*% diag(pca_factor$sdev[1:r])
M <- varimax(M)
M <- loadings(M)[1:ncol(poke_numeric), 1:r]
```

```{r}
plot(1:p,M[,1],pch=19,col="deepskyblue2",xlab="",ylab="Weights",main="Weights for the first factor")
abline(h=0)
text(1:p,M[,1],labels=names(poke_numeric),pos = 1,col="firebrick2",cex=0.75)
```

This factor has high weight for both `Defense` and `Sp.Def` as well as for `Attack`, `HP` and `Sp.Atk`. A possible interpretation of this latent variable is that it explains the general combat effectiveness of the pokemon, regardless of speed. A high value for this latent variable would mean for example that a pokemon would last longer in combat.

```{r}
plot(1:p,M[,2],pch=19,col="deepskyblue2",xlab="",ylab="Weights",main="Weights for the second factor")
abline(h=0)
text(1:p,M[,2],labels=names(poke_numeric),pos = 1,col="firebrick2",cex=0.75)
```

This factor has high (negative) weights for `Speed` and and other related offensive variables. A possible interpretation for this factor is the "slowness" of the pokemon.

### Commonalities and Uniqueness

We can compute the commonalities to see how well each variable is explained by the factors:

```{r}
comm <- diag(M %*% t(M))
sort(comm, decreasing = T)
```

The estimate of the covariance matrix of the errors is given by the following formula, which we will then use to compute the uniqueness of our variables. This will give us an estimate of the unexplained variance present in them.

```{r}
sigma_nu <- diag(diag(cov(poke_scaled) - M %*% t(M)))
uniq <- diag(sigma_nu)
names(uniq) <- names(comm)
sort(uniq, decreasing=TRUE)
```

We see that the better explained variables by the factors are `Speed` and `Defense`, while the worst ones are `HP` and `Attack`.

# Cluster Analysis

We will explore the results of the different cluster analysiss techniques seen in class on our dataset. We will use not the original dataset but the variables just related to one pokemon (because of the problems with using the full dataset seen in the previous section).

## K-Means Clustering

The first technique we will explore is k-means clustering. 

```{r}
kmeans_poke <- kmeans(poke_numeric, centers = 2)
```

We can plot the first two principal components divided by clusters:

```{r, fig.height = 3}
pca_poke <- prcomp(poke_numeric)
as_tibble(pca_poke$x[,1:2]) %>% 
  bind_cols(cluster = factor(kmeans_poke$cluster)) %>% 
  ggplot(aes(PC1,PC2, color = cluster))+
  geom_point()
```

For this dataset we had that the first PC explained something relative to the general power level of the pokemon, so a possible interpretation of this split is that the algorithm is a assigning one cluster to "weak" pokemon and another to "strong" pokemon in some general sense.

However having 2 clusters might not be the optimal split for our dataset. We can plot the `withinss` metric for several choices of $k$ to see which one is most appropiate.

```{r, fig.height=3}
fviz_nbclust(poke_numeric, kmeans, method = "wss", k.max = 10)
```

There appears to be an elbow around the $k=3$ or $k=4$, which suggests that these two could be used as an appropiate number of clusters. We can also use the silhouette method to get further confirmation for $k$.

```{r, fig.height=3}
fviz_nbclust(poke_numeric, kmeans, method = "silhouette", k.max = 10)
```

So actually the silhoutte method suggest that we take $k=2$, which was what we had donde previously. This certainly makes for an easier interpretation of the cluster, but we can check how the dataset looks with $k=3$ to see whether we get some relevant information.

```{r, fig.height=3}
kmeans_poke3 <- kmeans(poke_numeric, centers = 3)
pca_poke <- prcomp(poke_numeric)
as_tibble(pca_poke$x[,1:2]) %>% 
  bind_cols(cluster = factor(kmeans_poke3$cluster)) %>% 
  ggplot(aes(PC1,PC2, color = cluster))+
  geom_point()
```

So we can see that adding another cluster gives us some differences related to the second principal component, as it splits the dataset vertically. The second principal component of our dataset was related to the offensive capabilities of the pokemon, so the clustering algorithm could be distinguishing, among the stronger pokemon, those that are stronger offenseively from those that are stronger deffensively.

The silhouette plot for this case is

```{r, fig.height=4}
sil_kmeans3 <- silhouette(kmeans_poke3$cluster, dist(poke_numeric, method = "euclidean"))
plot(sil_kmeans3, col = "deepskyblue2", main = "Silhouette plot for K-Means (k=3)")
```

We can see that most observations have a positive silhouette, although there is a small number of them with negative or close to zero values.

## Partitioning Around Medioids (PAM)

We know that the k-means algorithm is quite sensitive to outliers, as it makes use of the sample mean vector. One solution for this problem is to use an algorithm that does not do clustering using the means but the *medioids* of the groups. The most common algorithm for doing this is the PAM algorithm.

```{r, fig.height=4}
pam_poke <- pam(poke_numeric, k=3, metric="euclidean", stand=FALSE)
fviz_cluster(pam_poke, data = pca_poke$x[,1:2], geom = "point")
```

We can see that the clusters in this case are mostly split along the first principal component. The silhouette for this case is:

```{r}
silhouette(pam_poke$clustering, dist(poke_numeric, method = "euclidean")) %>% 
  plot(main = "Silhouette plot for PAM (k=3)", col = "deepskyblue2")
```

So we can see that the average silhouette width is smaller than with kmeans. We can also see that the number of observations with a negative silhouette is bigger than in k-means, which indicates a worse fit for this algorithm. One alternative we can take with the PAM algorithm is to use the Manhattan distance instead of euclidean, which gives us the following result:

```{r, fig.height=4}
pam_poke_manh <- pam(poke_numeric, k=3, metric="manhattan", stand=FALSE)
fviz_cluster(pam_poke_manh, data = pca_poke$x[,1:2], geom = "point")

sil_pam_manh <- silhouette(pam_poke_manh$clustering, dist(poke_numeric, method = "manhattan"))
plot(sil_pam_manh, col = "deepskyblue2", main = "Silhouette plot for PAM (Manhattan Distance) (k=3)")
```

The average silhoutte for this case is `r mean(sil_pam_manh[,"sil_width"]) %>% round(5)`, which is closer to the one in kmeans but still not quite good enough.

## Agglomerative Hierarchical Clustering

A different approach to clustering, where starting from a cluster per single observation it progressively merges to a single cluster with all observations by selecting the closest cluster. Therefore, it is heavily influenced by the used distance.

Single Linkage selects the cluster by taking into consideration the minimum distance (Gower/Manhattan) to any observation of another cluster. That is, ignores the global distance of such cluster and measures single observations, producing huge clusters.

Since the dendogram is not useful given the number of observations we represent the first two components of our PCA using the number of clusters previously defined $k=3$. Being the result a clustering completely unbalanced

```{r fig.height=3}
poke_dists <- daisy(poke_numeric,metric="manhattan",stand=FALSE)

single <- hclust(poke_dists,method="single")
colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(single,3)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```

Complete Linkage on the contrary takes into account the maximum distance of the observations of the cluster to which it is being compared. Again we plot the first two principal components, improving the single linkage clustering but still not being properly defined.

```{r fig.height=3}
single <- hclust(poke_dists,method="complete")
colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(single,3)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```

The average linkage takes into consideration the mean distance (Manhattan) of the clusters. Still the result is not optimal.

```{r fig.height=3}
single <- hclust(poke_dists,method="average")
colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(single,3)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```

Ward Linkage again uses the squared Euclidean distance between the sample mean vector of the clusters. Producing a result closer to the one provided by kmeans and a better result in this case than the other types of linkage.

```{r fig.height=3}
single <- hclust(poke_dists,method="ward.D2")
colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(single,3)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```

## Divisive Hierarchical Clustering

On the contrary to agglomerative clustering divisive clustering starts from a single cluster and progressively divides it until there is a single cluster per observation by splitting the observations with the largest distance. Using manhattan distance with Divisive Analysis Clustering (DIANA) algorithm on our dataset we obtain a non-optimal division (with $k=3$), with a cluster with a single observation.

```{r fig.height=3}
diana_poke <- diana(poke_numeric,metric="manhattan")

colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(diana_poke,3)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```

Suggesting that a smaller number of clusters could improve the division. However, it worsens the result.

```{r fig.height=3}
colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(diana_poke,2)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```


Changing the distance used to euclidean provides us with a better and optimal result, again using $k=3$.

```{r fig.height=3}
diana_poke <- diana(poke_numeric,metric="euclidean")

colours_single <- c("deepskyblue2","firebrick2","orange")[cutree(diana_poke,3)]
plot(pca_poke$x[,1:2],pch=20,col=colours_single)
```


## Model-Based Clustering

Now we are going to perform a model-based approach. This technique attempts to fit a mixture of gaussians to our data. The number of gaussians and their shape have to be guessed, but we can compute the BIC for all the possible models to aid us in choosing the most appropiate configuration. There is a function that perform this process automatically.

```{r}
bic = mclustBIC(poke_numeric,G=1:9)

plot(bic)
```

The best possible model is the VVE with 4 components. This model correspond to the `ellipsoidal, equal orientation`. We can automatically compute it and plot it.

```{r}
model.cluster <- Mclust(poke_numeric,x=bic)

colors <- c("deepskyblue2","firebrick2","orange")[model.cluster$classification]
plot(pca_poke$x[,1:2],pch=20,col=colors)
```

As we can observe, the data does not look fitted very well. Nevertheless, from the previous plot, we can see that almost all types of mixtures behave similar when three components are present, so we can conclude that this kind of clustering does not behave well for our dataset.

## Conclusions

Overall the best performing cluster method appears to be k-means clustering with $k=3$. The cluster assignments visualized on the first two principal components was the following: 

```{r, fig.height = 3}
as_tibble(pca_poke$x[,1:2]) %>% 
  bind_cols(cluster = factor(kmeans_poke3$cluster)) %>% 
  ggplot(aes(PC1,PC2, color = cluster))+
  geom_point()
```

The clusters for this algorithm appear to be the most well separated, both visually and using the silhouette metric. However it is true that the principal component plot does not show evidence of there being any obvious clusters.
